{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Assignment — Part 1: Custom CNNs\n",
    " \n",
    "| | |\n",
    "|---|---|\n",
    "| **Students** | Gianluca Lascaro, Raffaele Rizzuti |\n",
    "| **University** | Universidad de A Coruña |\n",
    "| **Academic Year** | 2025–2026 |\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds \n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    " \n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameters & Configuration\n",
    "\n",
    "Centralising all configuration constants here makes it easy to tune them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Image settings ──────────────────────────────────────────────────────────\n",
    "TARGET_SIZE  = (96, 96)    # STL-10 native resolution; change if you resize\n",
    "NUM_CHANNELS = 3           # RGB\n",
    "NUM_CLASSES  = 10\n",
    "\n",
    "# ── Dataset split ───────────────────────────────────────────────────────────\n",
    "VALIDATION_SPLIT = 0.2     # 20% of training set → validation\n",
    "\n",
    "# ── Training settings (used later) ──────────────────────────────────────────\n",
    "BATCH_SIZE = 32\n",
    "AUTOTUNE   = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loading the STL-10 Dataset\n",
    "\n",
    "The **STL-10** dataset is loaded directly via TensorFlow Datasets. It includes:\n",
    "\n",
    "- 5,000 labeled training images (500 per class)  \n",
    "- 8,000 test images  \n",
    "- 100,000 unlabeled images (not used here)  \n",
    "\n",
    "Each image has size **96 × 96 × 3 (RGB)**.\n",
    "\n",
    "The code performs the following steps:\n",
    "\n",
    "- Loads the full dataset with metadata (`info`).  \n",
    "- Splits the labeled training set into:\n",
    "  - **Training:** 80% (4,000 images)  \n",
    "  - **Validation:** 20% (1,000 images)  \n",
    "- Loads the full test set (8,000 images).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset, info = tfds.load('stl10', with_info=True, as_supervised=True)\n",
    "\n",
    "# Split the dataset into training,validation and test sets\n",
    "train_split = tfds.load('stl10', split='train[:80%]', as_supervised=True)\n",
    "val_split = tfds.load('stl10', split='train[80%:]', as_supervised=True)\n",
    "test_split = tfds.load('stl10', split='test', as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Exploratory Data Analysis (EDA)\n",
    "\n",
    "A quick sanity-check before any preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Dataset sizes\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m num_train = \u001b[43mds_info\u001b[49m.splits[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m].num_examples\n\u001b[32m      3\u001b[39m num_test  = ds_info.splits[\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m].num_examples\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining samples : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_train\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'ds_info' is not defined"
     ]
    }
   ],
   "source": [
    "# Dataset sizes\n",
    "num_train = ds_info.splits['train'].num_examples\n",
    "num_test  = ds_info.splits['test'].num_examples\n",
    "print(f\"Training samples : {num_train}\")\n",
    "print(f\"Test samples     : {num_test}\")\n",
    "\n",
    "# Class names\n",
    "class_names = ds_info.features['label'].names\n",
    "print(f\"Classes ({NUM_CLASSES}): {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise a few raw samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
    "for ax, (image, label) in zip(axes.flat, ds_train_full.take(10)):\n",
    "    ax.imshow(image.numpy())\n",
    "    ax.set_title(class_names[label.numpy()], fontsize=9)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Sample raw images from STL-10 training set', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Preprocessing Pipeline\n",
    "\n",
    "We define a single `preprocess` function that will be mapped over every split.  \n",
    "Steps applied:\n",
    "\n",
    "1. **Resize** — standardise all images to `TARGET_SIZE`\n",
    "2. **Normalize** — scale pixel values from `[0, 255]` → `[0.0, 1.0]`\n",
    "3. **One-hot encode** — convert integer labels to categorical vectors of length `NUM_CLASSES`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, label):\n",
    "    \"\"\"Resize, normalize and one-hot encode a single (image, label) pair.\"\"\"\n",
    "    # 1. Standardise image size\n",
    "    image = tf.image.resize(image, TARGET_SIZE)\n",
    "\n",
    "    # 2. Normalize to [0, 1]\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "\n",
    "    # 3. One-hot encode the label\n",
    "    label = tf.one_hot(label, depth=NUM_CLASSES)\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train / Validation Split\n",
    "\n",
    "The STL-10 TFDS API does not provide a built-in validation split, so we carve \n",
    "one out of the training set manually using `take` and `skip`.\n",
    "\n",
    "Although STL-10 is perfectly balanced by construction (500 images per class),\n",
    "we shuffle **before** splitting to avoid any class-ordering artifacts that may\n",
    "arise from the way TFDS loads the dataset on disk, ensuring the validation set\n",
    "is a representative sample of the full training distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute split sizes\n",
    "num_val          = int(num_train * VALIDATION_SPLIT)\n",
    "num_train_split  = num_train - num_val\n",
    "\n",
    "print(f\"Train subset size      : {num_train_split}\")\n",
    "print(f\"Validation subset size : {num_val}\")\n",
    "\n",
    "# Shuffle BEFORE splitting to ensure class balance\n",
    "ds_train_full = ds_train_full.shuffle(\n",
    "    buffer_size=num_train, seed=SEED, reshuffle_each_iteration=False\n",
    ")\n",
    "\n",
    "ds_val   = ds_train_full.take(num_val)\n",
    "ds_train = ds_train_full.skip(num_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build Final tf.data Pipelines\n",
    "\n",
    "We apply preprocessing and configure efficient pipelines with batching and prefetching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = (\n",
    "    ds_train\n",
    "    .map(preprocess, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "ds_val = (\n",
    "    ds_val\n",
    "    .map(preprocess, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "ds_test = (\n",
    "    ds_test\n",
    "    .map(preprocess, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "print(\"Datasets ready:\")\n",
    "print(f\"  ds_train : {ds_train}\")\n",
    "print(f\"  ds_val   : {ds_val}\")\n",
    "print(f\"  ds_test  : {ds_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sanity Check — Preprocessed Samples\n",
    "\n",
    "Verify shapes, value ranges, and label format after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in ds_train.take(1):\n",
    "    print(f\"Image batch shape : {images.shape}\")\n",
    "    print(f\"Label batch shape : {labels.shape}\")\n",
    "    print(f\"Pixel min / max   : {images.numpy().min():.4f} / {images.numpy().max():.4f}\")\n",
    "    print(f\"Example label (one-hot): {labels[0].numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
    "for images, labels in ds_train.take(1):\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(images[i].numpy())\n",
    "        ax.set_title(class_names[np.argmax(labels[i].numpy())], fontsize=9)\n",
    "        ax.axis('off')\n",
    "plt.suptitle('Preprocessed training samples', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
